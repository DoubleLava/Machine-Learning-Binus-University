{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc6725d-db4e-4e86-8784-df4111998476",
   "metadata": {},
   "source": [
    "# Session 21-22 Support Vector Machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce6c410e-1a77-400c-902e-7f320aef07a2",
   "metadata": {},
   "source": [
    "# Exercise: Credit Card Fraud Detection\n",
    "\n",
    "You are a data scientist at a financial institution tasked with improving the company’s fraud detection system.\n",
    "\n",
    "You are given a large, high-dimensional transactional dataset that is suspected to contain redundant and irrelevant features.\n",
    "\n",
    "Dataset link:\n",
    "https://www.kaggle.com/datasets/shayannaveed/credit-card-fraud-detection\n",
    "\n",
    "Your objective is to apply dimensionality reduction techniques to simplify the data while preserving the most informative patterns for fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ff5ba-aeb6-43db-b356-aace6c81d704",
   "metadata": {},
   "source": [
    "# Step 1 — Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb789550-a205-40d9-8e3e-7a11b6785ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 1: DATA UNDERSTANDING\n",
    "# =========================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed4e5c-3a2d-4e49-aa85-dca02083addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Preview the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406499d-987a-42fc-9981-d53a6ee2c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution (fraud vs non-fraud)\n",
    "df[\"Class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d2e84-09c6-4843-a00f-ab8cd2fcd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(\"Class\", axis=1)  # All input features\n",
    "y = df[\"Class\"]               # Target variable (0 = non-fraud, 1 = fraud)\n",
    "\n",
    "# Print dataset shape\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec57a1-4b49-48a3-8fad-497f1088d9f4",
   "metadata": {},
   "source": [
    "## Why dimensionality reduction helps (conceptual):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafb215-99e2-4f3c-8ed4-21bf6d106ca7",
   "metadata": {},
   "source": [
    "This dataset has many features (V1–V28) already PCA-transformed, plus Time and Amount.\n",
    "\n",
    "Dimensionality reduction can:\n",
    "- Reduce noise\n",
    "- Improve computational efficiency\n",
    "- Help visualization\n",
    "- Potentially improve model generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c72161-0d75-40a5-9fd7-6966a0e0221c",
   "metadata": {},
   "source": [
    "# Step 2 — Method Selection\n",
    "\n",
    "We will use three methods for different purposes:\n",
    "\n",
    "| Method | Purpose |\n",
    "|-------|---------|\n",
    "| PCA | Feature compression for modeling |\n",
    "| t-SNE | Visualization (NOT modeling) |\n",
    "| LDA | Supervised dimensionality reduction |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cebb99-29a4-4d3b-9062-a586e623c3a2",
   "metadata": {},
   "source": [
    "# Step 3 — Dimensionality Reduction\n",
    "\n",
    "## 3.1 Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9650e-a5e3-4ed1-b797-090db3a382d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FEATURE SCALING\n",
    "# =========================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale features (VERY important for PCA, t-SNE, LDA)\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e32774-bf6c-428b-8312-49c5d01fcbdc",
   "metadata": {},
   "source": [
    "## 3.2 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b5ce7b-55a6-4a55-be55-b87adb441c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PCA\n",
    "# =========================\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Keep enough components to explain 95% of variance\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Original number of features:\", X_scaled.shape[1])\n",
    "print(\"Reduced number of features after PCA:\", X_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dfff14-7f8f-41b2-ba07-4d66ed717b34",
   "metadata": {},
   "source": [
    "### What does this line do?\n",
    "\n",
    "`pca = PCA(n_components=0.95, random_state=42)`\n",
    "\n",
    "This line creates a PCA model that automatically selects the minimum number of principal components needed to retain 95% of the variance in the data.\n",
    "\n",
    "`n_components = 0.95` : Keep enough principal components to explain 95% of the total variance.\n",
    "\n",
    "`random_state = 42` : Ensures reproducibility. PCA itself is deterministic, but some underlying numerical procedures may involve randomness. Setting `random_state` ensures the same result every time the code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82093af5-0d54-44f9-abce-3b9eb915ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance ratio\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Show cumulative variance\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0599e-5451-42ee-97db-796327238583",
   "metadata": {},
   "source": [
    "## 3.3 t-SNE (Visualization Only)\n",
    "t-SNE is not used for downstream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51328539-1071-43e3-8d2e-3ba00ce42ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# t-SNE (Visualization)\n",
    "# =========================\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Use a subset for speed (t-SNE is expensive)\n",
    "sample_idx = np.random.choice(len(X_scaled), size=5000, replace=False)\n",
    "\n",
    "X_subset = X_scaled[sample_idx]\n",
    "y_subset = y.iloc[sample_idx]\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate=200,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_tsne = tsne.fit_transform(X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a98a0-74e5-41d3-b2eb-72b4f3bc6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    X_tsne[:, 0],\n",
    "    X_tsne[:, 1],\n",
    "    c=y_subset,\n",
    "    cmap=\"coolwarm\",\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title(\"t-SNE Visualization of Credit Card Transactions\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.colorbar(label=\"Non-Fraud (0) / Fraud (1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b571e6-4a2f-4f91-9127-d371162da365",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "t-SNE preserves local neighborhoods.\n",
    "\n",
    "Clusters may appear well separated, but distances are NOT interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a3a3a-aad9-486e-aec0-01047560e74e",
   "metadata": {},
   "source": [
    "## 3.4 Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA is supervised, so it uses labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fe57a-009d-4915-92fa-58a934b1b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LDA\n",
    "# =========================\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# For binary classification, LDA can reduce to at most 1 component\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "print(\"LDA reduced shape:\", X_lda.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd88f0a-ded1-4164-b470-7fab48d963d9",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "LDA maximizes class separability.\n",
    "    \n",
    "Since this is a binary problem, max components = number_of_classes - 1 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12cbc1-c99c-42de-9b39-5866b1ccfab2",
   "metadata": {},
   "source": [
    "# Step 4 — Analysis and Interpretation\n",
    "\n",
    "## Information Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a8937-5a1b-4999-88d4-dc230123ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA information retention\n",
    "print(\"Total variance retained by PCA:\", explained_variance[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e62cf-3d96-422b-a597-3305b9193127",
   "metadata": {},
   "source": [
    "## Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49310d6c-7581-4ccc-b71f-742aba08f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# METHOD COMPARISON SUMMARY\n",
    "# =========================\n",
    "\n",
    "methods_summary = pd.DataFrame({\n",
    "    \"Method\": [\"Original\", \"PCA\", \"t-SNE\", \"LDA\"],\n",
    "    \"Dimensions\": [\n",
    "        X_scaled.shape[1],\n",
    "        X_pca.shape[1],\n",
    "        2,\n",
    "        X_lda.shape[1]\n",
    "    ],\n",
    "    \"Purpose\": [\n",
    "        \"Raw data\",\n",
    "        \"Modeling & compression\",\n",
    "        \"Visualization only\",\n",
    "        \"Supervised discrimination\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "methods_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd0e2a-6971-4e79-9422-ff04cba912fc",
   "metadata": {},
   "source": [
    "## Final Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb1500-b43f-4a9c-a855-cef991bf9aa1",
   "metadata": {},
   "source": [
    "PCA:\n",
    "- Reduces dimensionality while retaining variance\n",
    "- Suitable for fraud detection models\n",
    "\n",
    "t-SNE:\n",
    "- Excellent for visualization\n",
    "- NOT suitable for training classifiers\n",
    "\n",
    "LDA:\n",
    "- Uses class labels\n",
    "- Maximizes fraud vs non-fraud separation\n",
    "- Very useful for imbalanced classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfca9dd-09b0-41ad-b268-baddc2eb0216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
