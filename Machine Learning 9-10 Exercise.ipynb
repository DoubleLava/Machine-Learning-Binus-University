{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc6725d-db4e-4e86-8784-df4111998476",
   "metadata": {},
   "source": [
    "# Session 9-10 Model Selection and Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce6c410e-1a77-400c-902e-7f320aef07a2",
   "metadata": {},
   "source": [
    "# Exercise: Model complexity, cross-validation, and regularization\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "\n",
    "* Generate synthetic data for regression problems\n",
    "\n",
    "* Identify underfitting and overfitting using model complexity\n",
    "\n",
    "* Evaluate models using train‚Äìtest split and cross-validation\n",
    "\n",
    "* Apply regularization to control overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ff5ba-aeb6-43db-b356-aace6c81d704",
   "metadata": {},
   "source": [
    "# Part 1: Data Generation\n",
    "\n",
    "## Task 1.1 ‚Äì Create synthetic nonlinear regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47b914-46c4-425d-b9c2-6077e8840d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.1: Data generation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df960d-4d13-4c6b-a81e-a725561a6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate input feature x (continuous range between -1 and 1)\n",
    "n_samples = 30\n",
    "X = np.random.uniform(-1, 1, size=n_samples)\n",
    "\n",
    "# Define a nonlinear relationship (true function)\n",
    "y_true = 2 * X**2 + X\n",
    "\n",
    "# Add random Gaussian noise\n",
    "noise = np.random.normal(0, 0.2, size=n_samples)\n",
    "y = y_true + noise\n",
    "\n",
    "# Reshape X for sklearn (expects 2D array)\n",
    "X = X.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b2d32-3f93-4dd3-b7b1-ba09dbc0e342",
   "metadata": {},
   "source": [
    "## Task 1.2 ‚Äì Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af77f0-7501-4ec7-a6c4-129f2d9abc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.2: Visualization\n",
    "\n",
    "plt.scatter(X, y, color=\"blue\", label=\"Observed data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Synthetic Nonlinear Regression Dataset\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a820c160-3892-4687-9161-cdd775f3b5e1",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "#### Is a linear model sufficient to represent this data?\n",
    "\n",
    "No. The curved pattern indicates a nonlinear relationship, which a straight line cannot capture well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c28b46-9d1a-41fe-b188-8d30ac8b9a8a",
   "metadata": {},
   "source": [
    "# Part 2: Polynomial Regression & Model Complexity\n",
    "\n",
    "## Task 2.1 ‚Äì Train models with different polynomial degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e3a5c-0439-4e9d-ad2d-416bbb5561d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.1: Polynomial regression models\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e613e2-e58f-4c7d-9911-a6e036b7f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 3, 10]  # low, medium, high complexity\n",
    "models = {}\n",
    "\n",
    "for d in degrees:\n",
    "    poly = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    \n",
    "    models[d] = (poly, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56b8b1-4247-4d31-952f-47d925417313",
   "metadata": {},
   "source": [
    "## Task 2.2 ‚Äì Plot prediction curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f27db-4546-43f2-a2a2-355b1f7c956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.2: Plot prediction curves\n",
    "\n",
    "X_plot = np.linspace(-1, 1, 200).reshape(-1, 1)\n",
    "\n",
    "plt.scatter(X, y, color=\"black\", label=\"Data\")\n",
    "\n",
    "for d, (poly, model) in models.items():\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_plot = model.predict(X_plot_poly)\n",
    "    plt.plot(X_plot, y_plot, label=f\"Degree {d}\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Polynomial Regression with Different Degrees\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "535f5903-9467-4f1e-b30e-55bd72bccbb4",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "#### Which model is underfitting? Which model is overfitting? Which model appears to generalize best? Explain your reasoning based on the plots.\n",
    "\n",
    "* Underfitting: Degree 1\n",
    "\n",
    "* Overfitting: Degree 10\n",
    "\n",
    "* Best generalization: Degree 3\n",
    "\n",
    "Based on smoothness and ability to follow the trend without fitting noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b8791-1529-4925-91f7-dab842c87a1a",
   "metadata": {},
   "source": [
    "# Part 3: Train‚ÄìTest Split Evaluation\n",
    "\n",
    "## Task 3.1 ‚Äì Split data (70% train, 30% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a6054-6684-4d00-913e-5ae5dc8ea938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3.1: Train-test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d60ca32-caf1-4635-b63c-107c6017c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01dc44-cbbf-4cbf-95af-b6869ba1a56e",
   "metadata": {},
   "source": [
    "## Task 3.2 ‚Äì Compute training and test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04592349-b612-473e-8350-3f46eab346d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3.2: Evaluate each polynomial degree\n",
    "\n",
    "for d in degrees:\n",
    "    poly = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    \n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, model.predict(X_train_poly))\n",
    "    test_mse = mean_squared_error(y_test, model.predict(X_test_poly))\n",
    "    \n",
    "    print(f\"Degree {d}: Train MSE = {train_mse:.4f}, Test MSE = {test_mse:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe4154a1-8245-4b5e-9cf2-8563296a7a59",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "#### How do training and test errors differ for underfitting vs overfitting models?\n",
    "\n",
    "* Underfitting: High train & test error\n",
    "\n",
    "* Overfitting: Low train error, high test error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a8e68f-088c-4d19-aabd-063afe63b2b6",
   "metadata": {},
   "source": [
    "# Part 4: Cross-Validation\n",
    "\n",
    "## Task 4.1 ‚Äì 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715c014-627c-489b-8303-7cdc512e6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4.1: Cross-validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5d683-dc24-4ba2-a25d-f3e388acb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in degrees:\n",
    "    poly = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # Negative MSE is used by sklearn\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_poly, y,\n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "    \n",
    "    cv_mse = -cv_scores.mean()\n",
    "    print(f\"Degree {d}: CV MSE = {cv_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e98812-d467-4108-a16c-fb987d8a5a25",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "#### Why is cross-validation more reliable?\n",
    "\n",
    "It averages performance across multiple splits, reducing dependence on a single random train‚Äìtest split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53143eb-9bbf-43c7-880a-e0e84028da3c",
   "metadata": {},
   "source": [
    "# Part 5: Regularization\n",
    "\n",
    "## Task 5.1 ‚Äì Ridge Regression (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1cb38-f53b-4b34-81c4-a23b627c0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5.1: Ridge regression on high-degree model\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95d0c4-eea2-4b8b-bbc8-2fad128244b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 10\n",
    "lambdas = [0.01, 1, 100]\n",
    "\n",
    "poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "plt.scatter(X, y, color=\"black\", label=\"Data\")\n",
    "\n",
    "for lam in lambdas:\n",
    "    ridge = Ridge(alpha=lam)\n",
    "    ridge.fit(X_poly, y)\n",
    "    \n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_plot = ridge.predict(X_plot_poly)\n",
    "    \n",
    "    plt.plot(X_plot, y_plot, label=f\"Œª={lam}\")\n",
    "\n",
    "plt.title(\"Ridge Regression (L2 Regularization)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b88bb-4829-45a4-b5c4-00722a50574a",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "#### How does increasing ùúÜ affect: Model complexity? Bias and variance?\n",
    "\n",
    "Effect of increasing Œª\n",
    "\n",
    "* Lower complexity\n",
    "\n",
    "* Higher bias\n",
    "\n",
    "* Lower variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17bb61-1d61-4b4b-9724-0bb39b506d43",
   "metadata": {},
   "source": [
    "## Task 5.3 ‚Äì Lasso Regression (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a467b7a-57ef-4e56-8648-24b169b5518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5.3: Lasso regression\n",
    "\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c4378-6ee2-4780-aaa8-f73ffb798b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.05, max_iter=10000)\n",
    "lasso.fit(X_poly, y)\n",
    "\n",
    "# Inspect coefficients\n",
    "coefficients = lasso.coef_\n",
    "\n",
    "print(\"Lasso coefficients:\")\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782a71e-3075-4057-916c-ae114f815bd4",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "#### Which regularization method sets some coefficients exactly to zero?\n",
    "\n",
    "Lasso (L1)\n",
    "\n",
    "#### Why does this imply feature selection?\n",
    "\n",
    "Zero coefficients remove features entirely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef9667-bc8b-4db4-83d1-57cb1258fc14",
   "metadata": {},
   "source": [
    "# Part 6: Reflection (Answers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70330765-1694-4507-b264-18542eaf597a",
   "metadata": {},
   "source": [
    "#### (1) Why does increasing polynomial degree increase the risk of overfitting?\n",
    "\n",
    "Higher polynomial degree increases flexibility, allowing the model to fit noise.\n",
    "\n",
    "#### (2) How does regularization help control model complexity?\n",
    "\n",
    "Regularization penalizes large coefficients, limiting model complexity.\n",
    "\n",
    "#### (3) When would you prefer L1 over L2 regularization?\n",
    "\n",
    "L1 is preferred when feature selection or sparsity is desired.\n",
    "\n",
    "#### (4) Why is cross-validation important when tuning hyperparameters?\n",
    "\n",
    "Cross-validation provides a more stable estimate of model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
